{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04059328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35057586",
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62134022",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/om2/user/ckauf/.result_caching/neural_nlp.score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec8180aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_perturb_loss = ['original',\n",
    "                           'nouns',\n",
    "                           'nounsverbs',\n",
    "                           'nounsverbsadj',\n",
    "                           'contentwords',\n",
    "                           'random-nouns',\n",
    "                           'functionwords']\n",
    "\n",
    "categories_perturb_meaning = ['sentenceshuffle_passage',\n",
    "                              'sentenceshuffle_topic',\n",
    "                              'sentenceshuffle_random',\n",
    "                              'sentenceshuffle_random-topic-criteria',\n",
    "                              'sentenceshuffle_random-topic-length-criteria']\n",
    "\n",
    "categories_scrambled = ['original',\n",
    "                        'scrambled1',\n",
    "                        'scrambled3',\n",
    "                        'scrambled5',\n",
    "                        'scrambled7',\n",
    "                        'lowpmi',\n",
    "                        'lowpmi-random',\n",
    "                        'backward',\n",
    "                        'random-wl',\n",
    "                        'random-wl-samepos']\n",
    "\n",
    "categories_perturb = categories_perturb_loss + categories_perturb_meaning\n",
    "\n",
    "categories = [categories_perturb, categories_scrambled]\n",
    "keys = [\"perturb\", \"scrambled\"]\n",
    "\n",
    "categories_dict = dict(zip(keys, categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3addef1b",
   "metadata": {},
   "source": [
    "# Get best model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46770ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_score(matrix):\n",
    "    \"\"\"\n",
    "    input: result = out['data'].values matrix (e.g. for distilgpt2 a matrix of dimensions 7x2)\n",
    "    output: maximum score and associated error for this matrix.\n",
    "    \"\"\"\n",
    "    max_score, error = 0,0\n",
    "    for i in range(len(matrix)):\n",
    "        if matrix[i][0] > max_score:\n",
    "            max_score = matrix[i][0]\n",
    "            error = matrix[i][1]\n",
    "    return max_score, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bacf0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #not used currently\n",
    "# def get_best_score_dict(model_identifier):\n",
    "#     \"\"\"\n",
    "#     input: model_identifier\n",
    "#     output: dictionary containing the maximum score and associated error per condition.\n",
    "#             structure of dictionary: condition --> {score --> max_score, error --> associated error}\n",
    "#     \"\"\"\n",
    "#     score_dict = {}\n",
    "#     for filename in os.listdir(working_dir):\n",
    "#         model_name = filename.split(\",\")[1]\n",
    "#         if \"model=\" + model_identifier == model_name:\n",
    "#             if (\"-encoding-perturb\" in filename) or (\"encoding-scrambled\" in filename):\n",
    "#                 condition = filename.split(\",\")[0].split(\"Pereira2018-encoding-\")[-1]\n",
    "#                 print(condition)\n",
    "#                 if not condition in score_dict:\n",
    "#                     score_dict[condition] = {}\n",
    "#                 file = os.path.join(working_dir,filename)\n",
    "#                 with open(file, 'rb') as f:\n",
    "#                     out = pickle.load(f)\n",
    "#                 result = out['data'].values\n",
    "#                 #print(result, '\\n\\n')\n",
    "#                 max_score, error = get_max_score(result)\n",
    "#                 score_dict[condition]['score'] = max_score\n",
    "#                 score_dict[condition]['error'] = error\n",
    "#     return score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a110a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_scores_df(model_identifier,categories_dict,scr_perturb=None):\n",
    "    \"\"\"\n",
    "    input: model_identifier\n",
    "    output: dictionary containing the maximum score and associated error per condition.\n",
    "            structure of dictionary: condition --> {score --> max_score, error --> associated error}\n",
    "    \"\"\"\n",
    "    conditions = []\n",
    "    max_scores = []\n",
    "    errors = []\n",
    "    \n",
    "    for filename in os.listdir(working_dir):\n",
    "        if os.path.isdir(os.path.join(working_dir,filename)):\n",
    "            continue\n",
    "        model_name = filename.split(\",\")[1]\n",
    "        if \"model=\" + model_identifier == model_name:\n",
    "            if (f\"-encoding-{scr_perturb}\" in filename) or (\"-encoding-scrambled-original\" in filename):\n",
    "                \n",
    "                condition = filename.split(\",\")[0].split(\"Pereira2018-encoding-\")[-1]\n",
    "                \n",
    "                if \"perturb\" in scr_perturb:\n",
    "                    condition = re.sub(\"perturb-\",\"\",condition)\n",
    "                    condition = re.sub(\"scrambled-\",\"\",condition)\n",
    "                    \n",
    "                elif scr_perturb == \"scrambled\":\n",
    "                    if not any(x in condition for x in [\"1\", \"3\", \"5\", \"7\"]):\n",
    "                        condition = re.sub(\"scrambled-\",\"\",condition)\n",
    "                    else:\n",
    "                        condition = condition\n",
    "                #print(condition)\n",
    "\n",
    "                file = os.path.join(working_dir,filename)\n",
    "                with open(file, 'rb') as f:\n",
    "                    out = pickle.load(f)\n",
    "                result = out['data'].values\n",
    "                #print(result, '\\n\\n')\n",
    "                max_score, error = get_max_score(result)\n",
    "                \n",
    "                conditions.append(condition)\n",
    "                max_scores.append(max_score)\n",
    "                errors.append(error)\n",
    "                \n",
    "    import pandas as pd\n",
    "    index = conditions\n",
    "    df = pd.DataFrame({'score': max_scores,\n",
    "    'error': errors, 'condition': conditions})\n",
    "    df['condition'] = pd.Categorical(df['condition'], categories=categories_dict[scr_perturb])\n",
    "    scores_df = df.sort_values(by='condition')\n",
    "    return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f239d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_last_scores_df(model_identifier):\n",
    "#     \"\"\"\n",
    "#     input: model_identifier\n",
    "#     output: dictionary containing the last score and associated error per condition.\n",
    "#             structure of dictionary: condition --> {score --> max_score, error --> associated error}\n",
    "#     \"\"\"\n",
    "#     conditions = []\n",
    "#     last_scores = []\n",
    "#     errors = []\n",
    "#     for filename in os.listdir(working_dir):\n",
    "#         model_name = filename.split(\",\")[1]\n",
    "#         if \"model=\" + model_identifier == model_name:\n",
    "#             if (\"encoding-perturb\" in filename) or (\"encoding-scrambled\" in filename):\n",
    "#                 condition = filename.split(\",\")[0].split(\"Pereira2018-encoding-\")[-1]\n",
    "#                 condition = re.sub(\"ablation-\",\"\",condition)\n",
    "#                 condition = re.sub(\"scrambled-\",\"\",condition)\n",
    "\n",
    "#                 file = os.path.join(working_dir,filename)\n",
    "#                 with open(file, 'rb') as f:\n",
    "#                     out = pickle.load(f)\n",
    "#                 result = out['data'].values\n",
    "#                 #print(result, '\\n\\n')\n",
    "#                 last_score, error = last_score, error = result[-1][0], result[-1][1]\n",
    "#                 conditions.append(condition)\n",
    "#                 last_scores.append(last_score)\n",
    "#                 errors.append(error)\n",
    "#     import pandas as pd\n",
    "#     index = conditions\n",
    "#     df = pd.DataFrame({'score': last_scores,\n",
    "#     'error': errors, 'condition': conditions})\n",
    "#     df['condition'] = pd.Categorical(df['condition'], categories=['original', 'nouns', 'nounsverbs', 'nounsverbsadj', 'contentwords', 'functionwords', 'passageshuffle','nouns-delete50percent','random-lowPMI','random-nouns'])\n",
    "#     #df['condition'] = pd.Categorical(df['condition'], categories=['original', 'scrambled1', 'scrambled3', 'scrambled5', 'scrambled7', 'lowpmi', 'backward', 'random'])\n",
    "#     scores_df = df.sort_values(by='condition')\n",
    "#     return scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c0735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lookup from /om2/user/ckauf/anaconda/envs/perturbedenv/lib/python3.6/site-packages/brainio_collection/lookup.csv\n"
     ]
    }
   ],
   "source": [
    "#function check\n",
    "scores_df = get_best_scores_df(\"distilgpt2\",categories_dict,scr_perturb=\"scrambled\")\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc2530",
   "metadata": {},
   "source": [
    "# Prepare plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(model_identifier, scores_df, scr_perturb=None):\n",
    "    colors = sns.color_palette(\"rocket\")\n",
    "    x_pos = np.arange(len(scores_df))\n",
    "    scores = scores_df['score']\n",
    "    errors = scores_df['error']\n",
    "    conditions = scores_df['condition']\n",
    "    \n",
    "    # abbreviate condition names for perturb\n",
    "    new_conditions = []\n",
    "    for elm in conditions:\n",
    "        if \"sentenceshuffle_random\" in elm:\n",
    "            elm = re.sub(\"sentenceshuffle_random\",\"sent_random\",elm)\n",
    "        if \"topic-length-criteria\" in elm:\n",
    "            elm = re.sub(\"topic-length-criteria\",\"diffT-sameL\",elm)\n",
    "        elif \"topic-criteria\" in elm:\n",
    "            elm = re.sub(\"topic-criteria\",\"diffT\",elm)\n",
    "        if elm == \"sentenceshuffle_passage\":\n",
    "            elm = re.sub(\"sentenceshuffle_passage\",\"sent_passage\",elm)\n",
    "        if elm == \"sentenceshuffle_topic\":\n",
    "            elm = re.sub(\"sentenceshuffle_topic\",\"sent_topic\",elm)\n",
    "        new_conditions.append(elm)\n",
    "    conditions = new_conditions\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x_pos, scores,\n",
    "       yerr=errors,\n",
    "       align='center',\n",
    "       alpha=0.8, #color intensity\n",
    "       ecolor='black',\n",
    "       capsize=5, #error bar width\n",
    "       color=colors[1]) #if you put just color=colors, you get the bar plots in different rocket palette colors\n",
    "    ax.set_ylabel('ceiled score',fontsize=12)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(conditions, rotation=90)\n",
    "    \n",
    "    if len(model_identifier.split(\"-\")) == 1:\n",
    "        model_identifier += \"-lasttoken\"\n",
    "    ax.set_title('{}'.format(model_identifier),fontsize=18)\n",
    "    ax.yaxis.grid(True)\n",
    "    ax.set_ylim([0,1])\n",
    "    \n",
    "    if not os.path.isdir(\"fig\"):\n",
    "        os.mkdir(\"fig\")\n",
    "    if save:\n",
    "        savename = f'fig/match-to-brain_{scr_perturb}_{model_identifier}.png'\n",
    "        plt.savefig(savename, bbox_inches='tight', dpi=240)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_plot(model_identifier,categories_dict,scr_perturb=\"scrambled\"):\n",
    "    scores_df = get_best_scores_df(model_identifier,categories_dict,scr_perturb=scr_perturb)\n",
    "    bar_plot(model_identifier,scores_df,scr_perturb=scr_perturb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd47e5",
   "metadata": {},
   "source": [
    "# Ready to plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99328bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_plot('distilgpt2',categories_dict,scr_perturb=\"scrambled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b9774",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_plot('distilgpt2',categories_dict,scr_perturb=\"perturb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68673f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_plot('distilgpt2-avgtoken',categories_dict,scr_perturb=\"scrambled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1457a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_plot('distilgpt2-avgtoken',categories_dict,scr_perturb=\"perturb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6854b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_plot('gpt2',categories_dict,scr_perturb=\"perturb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_plot('gpt2-avgtoken',categories_dict,scr_perturb=\"perturb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_plot('gpt2',categories_dict,scr_perturb=\"scrambled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2808dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_plot('gpt2-avgtoken',categories_dict,scr_perturb=\"scrambled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf696d4",
   "metadata": {},
   "source": [
    "# Loop into subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb52492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def get_subplots_loop(model_list, scr_perturb=None):\n",
    "    nsubplots = len(model_list)\n",
    "    nrows = math.ceil(nsubplots/2)\n",
    "    colors = sns.color_palette(\"rocket\")\n",
    "    \n",
    "    fig2 = plt.figure(constrained_layout=True, figsize=(15, 6*nrows))\n",
    "    fig2.suptitle('{}'.format(scr_perturb), fontsize=22, y=1.05)\n",
    "    spec2 = GridSpec(ncols=2, nrows=nrows, figure=fig2)\n",
    "    f2_ax = []\n",
    "    \n",
    "    counter = 0\n",
    "    for i in range(nrows):\n",
    "        for j in range(2):\n",
    "            if counter + 1 > nsubplots:\n",
    "                break\n",
    "            else:\n",
    "                print(model_list[counter])\n",
    "                scores_df = get_best_scores_df(model_list[counter],categories_dict,scr_perturb=scr_perturb)\n",
    "                x_pos = np.arange(len(scores_df))\n",
    "                scores = scores_df['score']\n",
    "                errors = scores_df['error']\n",
    "                conditions = scores_df['condition']\n",
    "                \n",
    "                # abbreviate condition names for perturb\n",
    "                new_conditions = []\n",
    "                for elm in conditions:\n",
    "                    if \"sentenceshuffle_random\" in elm:\n",
    "                        elm = re.sub(\"sentenceshuffle_random\",\"sent_random\",elm)\n",
    "                    if \"topic-length-criteria\" in elm:\n",
    "                        elm = re.sub(\"topic-length-criteria\",\"diffT-sameL\",elm)\n",
    "                    elif \"topic-criteria\" in elm:\n",
    "                        elm = re.sub(\"topic-criteria\",\"diffT\",elm)\n",
    "                    if elm == \"sentenceshuffle_passage\":\n",
    "                        elm = re.sub(\"sentenceshuffle_passage\",\"sent_passage\",elm)\n",
    "                    if elm == \"sentenceshuffle_topic\":\n",
    "                        elm = re.sub(\"sentenceshuffle_topic\",\"sent_topic\",elm)\n",
    "                    new_conditions.append(elm)\n",
    "                conditions = new_conditions\n",
    "\n",
    "                f2_ax.append(fig2.add_subplot(spec2[i, j]))\n",
    "                f2_ax[-1].bar(x_pos, scores,\n",
    "                       yerr=errors,\n",
    "                       align='center',\n",
    "                       alpha=0.8, #color intensity\n",
    "                       ecolor='black',\n",
    "                       capsize=5, #error-bar width\n",
    "                       color=colors[1]) #if you put just color=colors, you get the bar plots in different rocket palette colors\n",
    "                f2_ax[-1].set_ylabel('ceiled score',fontsize=12)\n",
    "                f2_ax[-1].set_xticks(x_pos)\n",
    "                f2_ax[-1].set_xticklabels(conditions, rotation=90)\n",
    "                f2_ax[-1].set_ylim([0, 1])\n",
    "                \n",
    "                model_name = model_list[counter]\n",
    "                if len(model_name.split(\"-\")) == 1:\n",
    "                    model_name = model_name + \"-lasttoken\"\n",
    "                f2_ax[-1].set_title('{}'.format(model_name),fontsize=18)\n",
    "                f2_ax[-1].yaxis.grid(True)\n",
    "                counter += 1\n",
    "                \n",
    "    if save:\n",
    "        fig2.savefig(f'fig/match-to-brain_loop_{scr_perturb}.png', bbox_inches='tight', dpi=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05535ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subplots_loop(['distilgpt2', 'distilgpt2-avgtoken', 'gpt2', 'gpt2-avgtoken'], scr_perturb=\"perturb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_subplots_loop(['distilgpt2', 'distilgpt2-avgtoken', 'gpt2', 'gpt2-avgtoken'], scr_perturb=\"scrambled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c09b43c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
