{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b725d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d0de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = \"/om2/user/ckauf/.result_caching/neural_nlp.models.wrapper.core.ActivationsExtractorHelper._from_sentences_stored\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c62e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_scrambled = ['original',\n",
    "                        'Scr1',\n",
    "                        'Scr3',\n",
    "                        'Scr5',\n",
    "                        'Scr7',\n",
    "                        'lowPMI',\n",
    "                        'lowPMI-random',\n",
    "                        'backward',\n",
    "                        'random-wl']\n",
    "\n",
    "categories_perturb_loss = ['original',\n",
    "                           'nouns',\n",
    "                           'nounsverbs',\n",
    "                           'nounsverbsadj',\n",
    "                           'contentwords',\n",
    "                           'random-nouns',\n",
    "                           'functionwords']\n",
    "\n",
    "categories_perturb_meaning = ['sentenceshuffle_passage',\n",
    "                              'sentenceshuffle_topic',\n",
    "                              #'sentenceshuffle_random',\n",
    "                              #'sentenceshuffle_random-topic-criteria',\n",
    "                              'sentenceshuffle_random-topic-length-criteria']\n",
    "\n",
    "categories_perturb = categories_perturb_loss + categories_perturb_meaning\n",
    "\n",
    "categories = [categories_perturb, categories_scrambled]\n",
    "keys = [\"perturb\", \"scrambled\"]\n",
    "\n",
    "categories_dict = dict(zip(keys, categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78044bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_layers(model_identifier,categories_dict,scr_perturb=None):\n",
    "    \"\"\"\n",
    "    input: model_identifier of model of which we want to find the layers\n",
    "    output: np.array of all unique layer identifiers, ordered by position\n",
    "    \"\"\"\n",
    "    for ind,filename in enumerate(os.listdir(working_dir)):\n",
    "        if \"identifier=\" + model_identifier in filename:\n",
    "            if any(substring in filename for substring in categories_dict[scr_perturb]):\n",
    "                file = os.path.join(working_dir,filename)\n",
    "                with open(file, 'rb') as f:\n",
    "                    result = pickle.load(f)\n",
    "                result = result['data']\n",
    "                layer_list = np.unique(result.layer)\n",
    "                #order double-digit layers at end of list\n",
    "                double_digits = [elm for elm in layer_list if 'encoder.h.' in elm and len(elm.split('.h.')[-1]) > 1]\n",
    "                layers = [e for e in layer_list if e not in double_digits] + double_digits\n",
    "                return layers\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fa30e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_identifier(filename):\n",
    "    \"\"\"\n",
    "    get passage identifier to be used as key for the dictionary.\n",
    "    important step: fill the identifier with 0s for single-digit passage numbers\n",
    "    \"\"\"\n",
    "    passage = filename.split(\"-\")[-1].split(\".\")[0]\n",
    "    number = passage.split(\"sentences\")[-1]\n",
    "    if len(number) == 1:\n",
    "        passage_identifier = passage[:-1] + number.zfill(2)\n",
    "    else:\n",
    "        passage_identifier = passage\n",
    "    return passage_identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9f2dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictionary(model_identifier,categories_dict,scr_perturb=None,layer_identifier=None, emb_context=\"Passage\"):\n",
    "    \"\"\"\n",
    "    input: model identifier\n",
    "    output: populated model dictionary with data of just the selected layer\n",
    "    dictionary structure: passage_identifier --> condition identifier --> data\n",
    "    \"\"\"\n",
    "    \n",
    "    model_dictionary = model_identifier + \"_dict\"\n",
    "    model_dictionary = {}\n",
    "    \n",
    "    #look at last layer by default\n",
    "    if layer_identifier == None:\n",
    "        layer_identifier = get_last_layer(model_identifier,categories_dict,scr_perturb=scr_perturb)\n",
    "    else:\n",
    "        layer_identifier = layer_identifier\n",
    "    print(\"This is the layer I'm looking at: \", layer_identifier)\n",
    "    \n",
    "    for filename in tqdm(os.listdir(working_dir)):\n",
    "        model_name = filename.split(\",\")[0]\n",
    "        if (\"identifier=\" + model_identifier == model_name):\n",
    "            if (any(substring in filename for substring in categories_dict[scr_perturb])) or (\"Original\" in filename):\n",
    "                passage_identifier = get_passage_identifier(filename)\n",
    "\n",
    "                condition = filename.split(\"Pereira2018\")[1]\n",
    "                if \"avgtoken\" in model_identifier:\n",
    "                    condition = condition.split(\"avgtoken\")[0]\n",
    "                else:\n",
    "                    condition = condition.split(\"lasttoken\")[0]\n",
    "                condition = condition.lstrip(\"-\").rstrip(\"-\")\n",
    "\n",
    "                file = os.path.join(working_dir,filename)\n",
    "                with open(file, 'rb') as f:\n",
    "                    out = pickle.load(f)\n",
    "                result = out['data']\n",
    "                data = result[{\"neuroid\": [layer == layer_identifier for layer in result[\"layer\"].values]}]\n",
    "\n",
    "                if not passage_identifier in model_dictionary:\n",
    "                    model_dictionary[passage_identifier] = {}\n",
    "                model_dictionary[passage_identifier][condition] = data\n",
    "    \n",
    "    return model_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eb536ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arrays(dictionary,categories_dict,scr_perturb=None):\n",
    "    #sort dictionary by passage identifier\n",
    "    sorted_dict = dict(sorted(dictionary.items()))\n",
    "    #create empty arrays\n",
    "    \n",
    "    if scr_perturb == \"scrambled\":\n",
    "        original, scrambled1, scrambled3, scrambled5, scrambled7, lowpmi, lowpmi_random, backward, random_wl, random_wl_samepos = ([] for i in range(len(categories_dict[scr_perturb])))\n",
    "        for key, value in sorted_dict.items(): #key is passage, value is dic from cond to xarray data\n",
    "            #print(list(value.keys()))\n",
    "            original.append(value['Original'].values)\n",
    "            scrambled1.append(value['Scr1'].values)\n",
    "            scrambled3.append(value['Scr3'].values)\n",
    "            scrambled5.append(value['Scr5'].values)\n",
    "            scrambled7.append(value['Scr7'].values)\n",
    "            lowpmi.append(value['lowPMI'].values)\n",
    "            lowpmi_random.append(value['lowPMI-random'].values)\n",
    "            backward.append(value['backward'].values)\n",
    "            random_wl.append(value['random-wl'].values)\n",
    "            random_wl_samepos.append(value['random-wl-samepos'].values)\n",
    "        \n",
    "        return original,scrambled1,scrambled3,scrambled5,scrambled7,lowpmi,lowpmi_random,backward,random_wl,random_wl_samepos\n",
    "            \n",
    "    elif scr_perturb == \"perturb\":\n",
    "        original,nouns,nounsverbs,nounsverbsadj,contentwords,random_nouns,functionwords,sent_passage,sent_topic,sent_random = ([] for i in range(len(categories_dict[scr_perturb])))\n",
    "        #\n",
    "        for key, value in sorted_dict.items(): #key is passage, value is dic from cond to xarray data\n",
    "            #print(list(value.keys()))\n",
    "            original.append(value['Original'].values)\n",
    "            nouns.append(value['nouns'].values)\n",
    "            nounsverbs.append(value['nounsverbs'].values)\n",
    "            nounsverbsadj.append(value['nounsverbsadj'].values)\n",
    "            contentwords.append(value['contentwords'].values)\n",
    "            random_nouns.append(value['random-nouns'].values)\n",
    "            functionwords.append(value['functionwords'].values)\n",
    "            sent_passage.append(value['sentenceshuffle_passage'].values)\n",
    "            sent_topic.append(value['sentenceshuffle_topic'].values)\n",
    "            sent_random.append(value['sentenceshuffle_random'].values)\n",
    "            \n",
    "        \n",
    "        #print(np.shape(original))\n",
    "        #print(np.shape(original[0]))\n",
    "        return original,nouns,nounsverbs,nounsverbsadj,contentwords,random_nouns,functionwords,sent_passage,sent_topic,sent_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed8b8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_array(liste):\n",
    "    liste_flatten = [item for sublist in liste for item in sublist]\n",
    "    return liste_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27694e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_perturb(original,nouns,nounsverbs,nounsverbsadj,contentwords,random_nouns,functionwords,sent_passage,sent_topic,sent_random,flatten=True):\n",
    "    \n",
    "    if flatten:\n",
    "        df = pd.DataFrame(data={'original':np.asarray(flatten_array(original)).flatten(),\n",
    "                               'nouns':np.asarray(flatten_array(nouns)).flatten(),\n",
    "                               'nounsverbs':np.asarray(flatten_array(nounsverbs)).flatten(),\n",
    "                               'nounsverbsadj':np.asarray(flatten_array(nounsverbsadj)).flatten(),\n",
    "                               'contentwords':np.asarray(flatten_array(contentwords)).flatten(),\n",
    "                               'random-nouns' :np.asarray(flatten_array(random_nouns)).flatten(),\n",
    "                               'functionwords':np.asarray(flatten_array(functionwords)).flatten(),\n",
    "                               'sent_passage': np.asarray(flatten_array(sent_passage)).flatten(),\n",
    "                               'sent_topic': np.asarray(flatten_array(sent_topic)).flatten(),\n",
    "                               'sent_random' : np.asarray(flatten_array(sent_random)).flatten()\n",
    "                               })\n",
    "    if not flatten:\n",
    "        df = {}\n",
    "        df = {'original':flatten_array(original),\n",
    "             'nouns':flatten_array(nouns),\n",
    "              'nounsverbs':flatten_array(nounsverbs),\n",
    "              'nounsverbsadj':flatten_array(nounsverbsadj),\n",
    "              'contentwords':flatten_array(contentwords),\n",
    "              'random-nouns' : flatten_array(random_nouns),\n",
    "              'functionwords':flatten_array(functionwords),\n",
    "              'sent_passage':flatten_array(sent_passage),\n",
    "              'sent_topic': flatten_array(sent_topic),\n",
    "              'sent_random': flatten_array(sent_random)}\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17f8826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe_scrambled(original,scrambled1,scrambled3,scrambled5,scrambled7,lowpmi,lowpmi_random,backward,random_wl,random_wl_samepos,flatten=True):\n",
    "    \n",
    "    if flatten:\n",
    "        df = pd.DataFrame(data={'original':np.asarray(flatten_array(original)).flatten(),\n",
    "                               'scrambled1':np.asarray(flatten_array(scrambled1)).flatten(),\n",
    "                              'scrambled3' :np.asarray(flatten_array(scrambled3)).flatten(),\n",
    "                              'scrambled5' :np.asarray(flatten_array(scrambled5)).flatten(),\n",
    "                              'scrambled7':np.asarray(flatten_array(scrambled7)).flatten(),\n",
    "                              'lowpmi':np.asarray(flatten_array(lowpmi)).flatten(),\n",
    "                              'lowpmi_random':np.asarray(flatten_array(lowpmi_random)).flatten(),\n",
    "                              'backward':np.asarray(flatten_array(backward)).flatten(),\n",
    "                              'random-wl':np.asarray(flatten_array(random_wl)).flatten(),\n",
    "                              'random-wl_samepos':np.asarray(flatten_array(random_wl_samepos)).flatten()\n",
    "                               })\n",
    "    if not flatten:\n",
    "        # print(np.shape(np.asarray(flatten_array(random))))\n",
    "        df = {}\n",
    "        df = {'original':flatten_array(original),\n",
    "             'scrambled1':flatten_array(scrambled1),\n",
    "              'scrambled3' : flatten_array(scrambled3),\n",
    "              'scrambled5' : flatten_array(scrambled5),\n",
    "              'scrambled7':flatten_array(scrambled7),\n",
    "              'lowpmi':flatten_array(lowpmi),\n",
    "              'lowpmi_random':flatten_array(lowpmi_random),\n",
    "              'backward':flatten_array(backward),\n",
    "              'random-wl':flatten_array(random_wl),\n",
    "              'random-wl_samepos':flatten_array(random_wl_samepos)\n",
    "             }\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e0c8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_df_for_plotting(model_identifier, scr_perturb=None, layer_identifier=None, emb_context=\"Passage\", flatten=True):\n",
    "    model_dict = get_dictionary(model_identifier, categories_dict, scr_perturb=scr_perturb, layer_identifier=layer_identifier,\n",
    "                               emb_context=emb_context)\n",
    "    if scr_perturb == \"perturb\":\n",
    "        df = prepare_dataframe_perturb(*get_arrays(model_dict,categories_dict,scr_perturb=scr_perturb), flatten=flatten) #*flattens the tuple\n",
    "    elif scr_perturb == \"scrambled\":\n",
    "        df = prepare_dataframe_scrambled(*get_arrays(model_dict,categories_dict,scr_perturb=scr_perturb), flatten=flatten) #*flattens the tuple\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff2e075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_dfs_for_all_layers(model_identifier, categories_dict, scr_perturb=None, emb_context=\"Passage\", flatten=True):\n",
    "    \"\"\"\n",
    "    input: model_identifier, whether to flatten (i.e. all sentence reps in one vector or leave 627*hidden size)\n",
    "    output: dictionary: layer --> dataframe (conditions as column names, column values are flattened or unflattened activations)\n",
    "    \"\"\"\n",
    "    layers = get_all_layers(model_identifier,categories_dict,scr_perturb=scr_perturb)\n",
    "    print(f\"Available layers: {layers}\")\n",
    "    df_dict = {}\n",
    "    for ind,elm in enumerate(layers):\n",
    "        df_dict[elm] = main_df_for_plotting(model_identifier,scr_perturb=scr_perturb,layer_identifier=elm,\n",
    "                                            emb_context=emb_context, flatten=flatten)\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cfeade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlations_df_dict(model_identifier, categories_dict, scr_perturb=None, emb_context=\"Passage\"): #maybe pass activations dict as input so it doesn't have to recompute\n",
    "    layers = get_all_layers(model_identifier, categories_dict, scr_perturb=scr_perturb)\n",
    "    activations_dict = get_activation_dfs_for_all_layers(model_identifier, categories_dict, scr_perturb=scr_perturb, emb_context=emb_context)\n",
    "    \n",
    "    conditions = list(activations_dict[layers[0]].columns)\n",
    "    \n",
    "    correlations_df_dict = {}\n",
    "    for layer in layers:\n",
    "        orig_column = activations_dict[layer]['original']\n",
    "        correlations = [orig_column.corr(activations_dict[layer][elm]) for elm in conditions]\n",
    "        correlations_df_dict[layer] = correlations\n",
    "    \n",
    "    return layers, conditions, correlations_df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "070e081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlations_lineplot(model_identifier, layers, conditions, correlations_dict,scr_perturb=None,\n",
    "                              emb_context=\"Passage\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    line_colors = sns.color_palette(\"rocket\") + sns.color_palette(\"GnBu_d\") + sns.color_palette(\"PRGn\", 10) + sns.color_palette(\"YlOrBr\", 10)\n",
    "    \n",
    "    layers = layers\n",
    "    conditions = conditions\n",
    "    \n",
    "    counter = 0\n",
    "    for key,value in correlations_dict.items():\n",
    "        ax.plot(conditions,value, '-o',color=line_colors[counter])\n",
    "        counter += 1\n",
    "\n",
    "    ax.set_title(f'{model_identifier} | Layer model activation correlation with model activations for original sentence across conditions')\n",
    "    ax.legend(layers)\n",
    "    ax.yaxis.set_label_text('Pearson p')\n",
    "    ax.legend(layers, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.set_ylim([0, 1])\n",
    "    plt.xticks(rotation= 90)\n",
    "    \n",
    "    if not os.path.isdir(\"fig\"):\n",
    "        os.mkdir(\"fig\")\n",
    "    savename = f'fig/{date.today()}_model-activation-correlation_{model_identifier}_{scr_perturb}.png'\n",
    "    #plt.savefig(savename, bbox_inches='tight', dpi=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d0f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/ckauf/anaconda39/envs/perturbed3.8/lib/python3.8/site-packages/xarray/core/merge.py:17: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  PANDAS_TYPES = (pd.Series, pd.DataFrame, pd.Panel)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available layers: ['drop', 'encoder.h.0', 'encoder.h.1', 'encoder.h.2', 'encoder.h.3', 'encoder.h.4', 'encoder.h.5', 'encoder.h.6', 'encoder.h.7', 'encoder.h.8', 'encoder.h.9', 'encoder.h.10', 'encoder.h.11', 'encoder.h.12', 'encoder.h.13', 'encoder.h.14', 'encoder.h.15', 'encoder.h.16', 'encoder.h.17', 'encoder.h.18', 'encoder.h.19', 'encoder.h.20', 'encoder.h.21', 'encoder.h.22', 'encoder.h.23', 'encoder.h.24', 'encoder.h.25', 'encoder.h.26', 'encoder.h.27', 'encoder.h.28', 'encoder.h.29', 'encoder.h.30', 'encoder.h.31', 'encoder.h.32', 'encoder.h.33', 'encoder.h.34', 'encoder.h.35', 'encoder.h.36', 'encoder.h.37', 'encoder.h.38', 'encoder.h.39', 'encoder.h.40', 'encoder.h.41', 'encoder.h.42', 'encoder.h.43', 'encoder.h.44', 'encoder.h.45', 'encoder.h.46', 'encoder.h.47']\n",
      "This is the layer I'm looking at:  drop\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215f15a28529412da1f8f2205c3373e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers, conditions, corr_dict = get_correlations_df_dict(\"gpt2-xl\",categories_dict, scr_perturb=\"scrambled\", emb_context=\"Passage\")\n",
    "plot_correlations_lineplot(\"gpt2-xl\", layers, conditions, corr_dict, scr_perturb=\"scrambled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b2a7af",
   "metadata": {},
   "source": [
    "# Loop over models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "def get_corr_lineplots_loop(model_list, scr_perturb=None, emb_context=\"Passage\"):\n",
    "    nsubplots = len(model_list)\n",
    "    nrows = math.ceil(nsubplots/2)\n",
    "    #line_colors = sns.color_palette(\"rocket\") + [sns.color_palette(\"PRGn\", 10)[2]] + [sns.color_palette(\"PuOr\", 10)[0]] + sns.color_palette(\"GnBu_d\")\n",
    "    #line_colors = sns.color_palette(\"rocket\") + sns.color_palette(\"GnBu_d\") + [sns.color_palette(\"PRGn\", 10)[2]] + [sns.color_palette(\"PuOr\", 10)[0]]\n",
    "    #if model_identifier in ['xlnet-large-cased','bert-large-uncased-whole-word-masking']:\n",
    "    #    line_colors = sns.color_palette(\"rocket\") + sns.color_palette(\"GnBu_d\") + sns.color_palette(\"PRGn\", 10) + sns.color_palette(\"YlOrBr\", 10)\n",
    "    \n",
    "    \n",
    "    fig2 = plt.figure(constrained_layout=True, figsize=(15, 5*nrows))\n",
    "    fig2.suptitle(f'Layer model activation correlation with model activations for unperturbed sentence across conditions | {scr_perturb}', fontsize=22, y=1.05)\n",
    "    spec2 = GridSpec(ncols=2, nrows=nrows, figure=fig2)\n",
    "    f2_ax = []\n",
    "    \n",
    "    model_counter = 0\n",
    "    for i in range(nrows):\n",
    "        for j in range(2):\n",
    "            if model_counter + 1 > nsubplots:\n",
    "                break\n",
    "            else:\n",
    "                model_identifier = model_list[model_counter]\n",
    "                line_colors = sns.color_palette(\"rocket\") + sns.color_palette(\"GnBu_d\") + [sns.color_palette(\"PRGn\", 10)[2]] + [sns.color_palette(\"PuOr\", 10)[0]]\n",
    "                if model_identifier in ['xlnet-large-cased','bert-large-uncased-whole-word-masking']:\n",
    "                    line_colors = sns.color_palette(\"rocket\") + sns.color_palette(\"GnBu_d\") + sns.color_palette(\"PRGn\", 10) + sns.color_palette(\"YlOrBr\", 10)\n",
    "                layers, conditions, correlations_df_dict = get_correlations_df_dict(model_identifier,categories_dict, scr_perturb=scr_perturb)\n",
    "                f2_ax.append(fig2.add_subplot(spec2[i, j]))\n",
    "                \n",
    "                counter = 0\n",
    "                for key,value in correlations_df_dict.items():\n",
    "                    f2_ax[-1].plot(conditions,value, '-o',color=line_colors[counter])\n",
    "                    counter += 1\n",
    "                if len(model_identifier.split(\"-\")) == 1:\n",
    "                    model_identifier = model_identifier + \"-lasttoken\"\n",
    "                f2_ax[-1].set_title(f'{model_identifier}',fontsize=18)\n",
    "                f2_ax[-1].yaxis.set_label_text('Pearson p')\n",
    "                f2_ax[-1].legend(layers, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                f2_ax[-1].set_ylim([0, 1])\n",
    "                plt.xticks(rotation= 90)\n",
    "                model_counter += 1\n",
    "    fig2.savefig(f'fig/{date.today()}_activations-layer-comparison_loop_{scr_perturb}.png', bbox_inches='tight', dpi=240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e638a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_corr_lineplots_loop([\"gpt2-xl\"], scr_perturb=\"perturb\", emb_context=\"Passage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7a24f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_corr_lineplots_loop([\"distilgpt2\", \"distilgpt2-avgtoken\", \"gpt2\", \"gpt2-avgtoken\"], scr_perturb=\"scrambled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8248a92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
