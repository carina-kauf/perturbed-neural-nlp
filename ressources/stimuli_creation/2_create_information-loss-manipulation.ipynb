{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for creating information-loss manipulation datasets from the Pereira2018 fMRI stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from os.path import abspath\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import csv\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rdma/vast-rdma/vast/cpl/ckauf/perturbed-neural-nlp/ressources/stimuli_creation\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rdma/vast-rdma/vast/cpl/ckauf/perturbed-neural-nlp\n"
     ]
    }
   ],
   "source": [
    "importpath = abspath('../..')\n",
    "os.chdir(importpath)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add seeds for reproducability\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load base stimulus dataframe (Pereira 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading lookup from /om2/user/ckauf/anaconda/envs/perturbedenv/lib/python3.6/site-packages/brainio_collection/lookup.csv\n",
      "/rdma/vast-rdma/vast/cpl/ckauf/perturbed-neural-nlp/neural_nlp/../ressources/stimuli\n",
      "\n",
      "\n",
      " We're running in the NEW version of the implementations.py script.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/ckauf/anaconda/envs/perturbedenv/lib/python3.6/site-packages/brainio_base/assemblies.py:224: FutureWarning: The inplace argument has been deprecated and will be removed in a future version of xarray.\n",
      "  xr_data.set_index(append=True, inplace=True, **coords_d)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>story</th>\n",
       "      <th>passage_index</th>\n",
       "      <th>passage_label</th>\n",
       "      <th>passage_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beekeeping encourages the conservation of loca...</td>\n",
       "      <td>0</td>\n",
       "      <td>243sentences.0</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is in every beekeeper's interest to conserv...</td>\n",
       "      <td>1</td>\n",
       "      <td>243sentences.1</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a passive form of agriculture, it does not ...</td>\n",
       "      <td>2</td>\n",
       "      <td>243sentences.2</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beekeepers also discourage the use of pesticid...</td>\n",
       "      <td>3</td>\n",
       "      <td>243sentences.3</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artisanal beekeepers go to extremes for their ...</td>\n",
       "      <td>4</td>\n",
       "      <td>243sentences.4</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>2</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>Some windows have multiple panes to increase i...</td>\n",
       "      <td>379</td>\n",
       "      <td>384sentences.379</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.building_part</td>\n",
       "      <td>95</td>\n",
       "      <td>Window</td>\n",
       "      <td>building_part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>A woman is a female human adult.</td>\n",
       "      <td>380</td>\n",
       "      <td>384sentences.380</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>A woman is stereotypically seen as a caregiver.</td>\n",
       "      <td>381</td>\n",
       "      <td>384sentences.381</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>A woman can become pregnant and bear children.</td>\n",
       "      <td>382</td>\n",
       "      <td>384sentences.382</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>A woman has different reproductive organs than...</td>\n",
       "      <td>383</td>\n",
       "      <td>384sentences.383</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  sentence_num  \\\n",
       "0    Beekeeping encourages the conservation of loca...             0   \n",
       "1    It is in every beekeeper's interest to conserv...             1   \n",
       "2    As a passive form of agriculture, it does not ...             2   \n",
       "3    Beekeepers also discourage the use of pesticid...             3   \n",
       "4    Artisanal beekeepers go to extremes for their ...             4   \n",
       "..                                                 ...           ...   \n",
       "622  Some windows have multiple panes to increase i...           379   \n",
       "623                   A woman is a female human adult.           380   \n",
       "624    A woman is stereotypically seen as a caregiver.           381   \n",
       "625     A woman can become pregnant and bear children.           382   \n",
       "626  A woman has different reproductive organs than...           383   \n",
       "\n",
       "          stimulus_id    experiment                       story  \\\n",
       "0      243sentences.0  243sentences     243sentences.beekeeping   \n",
       "1      243sentences.1  243sentences     243sentences.beekeeping   \n",
       "2      243sentences.2  243sentences     243sentences.beekeeping   \n",
       "3      243sentences.3  243sentences     243sentences.beekeeping   \n",
       "4      243sentences.4  243sentences     243sentences.beekeeping   \n",
       "..                ...           ...                         ...   \n",
       "622  384sentences.379  384sentences  384sentences.building_part   \n",
       "623  384sentences.380  384sentences          384sentences.human   \n",
       "624  384sentences.381  384sentences          384sentences.human   \n",
       "625  384sentences.382  384sentences          384sentences.human   \n",
       "626  384sentences.383  384sentences          384sentences.human   \n",
       "\n",
       "     passage_index passage_label passage_category  \n",
       "0                1    beekeeping       beekeeping  \n",
       "1                1    beekeeping       beekeeping  \n",
       "2                1    beekeeping       beekeeping  \n",
       "3                1    beekeeping       beekeeping  \n",
       "4                2    beekeeping       beekeeping  \n",
       "..             ...           ...              ...  \n",
       "622             95        Window    building_part  \n",
       "623             96         Woman            human  \n",
       "624             96         Woman            human  \n",
       "625             96         Woman            human  \n",
       "626             96         Woman            human  \n",
       "\n",
       "[627 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neural_nlp.benchmarks.neural import *\n",
    "import neural_nlp\n",
    "from neural_nlp.stimuli import StimulusSet\n",
    "import xarray as xr\n",
    "\n",
    "benchmark_pool = [\n",
    "    # primary benchmarks\n",
    "    ('Pereira2018-encoding', PereiraEncoding),\n",
    "]\n",
    "benchmark_pool = {identifier: LazyLoad(lambda identifier=identifier, ctr=ctr: ctr(identifier=identifier))\n",
    "                  for identifier, ctr in benchmark_pool}\n",
    "\n",
    "# fetch stimulus set\n",
    "benchmark = benchmark_pool['Pereira2018-encoding']\n",
    "stimuli_df = benchmark._target_assembly.attrs['stimulus_set']\n",
    "stimuli_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coordinates:\n",
       "  * presentation      (presentation) MultiIndex\n",
       "  - stimulus_num      (presentation) int64 0 0 1 1 2 2 3 ... 12 12 13 13 14 14\n",
       "  - passage_index     (presentation) int64 1 1 1 1 1 1 1 1 2 ... 3 4 4 4 4 4 4 4\n",
       "  - passage_label     (presentation) object 'Accordion' ... 'dreams'\n",
       "  - passage_category  (presentation) object 'music' 'beekeeping' ... 'dreams'\n",
       "  - stimulus_id       (presentation) object '384sentences.0' ... '243sentences.14'\n",
       "  - story             (presentation) object '384sentences.music' ... '243sentences.dreams'\n",
       "  - experiment        (presentation) object '384sentences' ... '243sentences'\n",
       "  * neuroid           (neuroid) MultiIndex\n",
       "  - subject           (neuroid) object '018' '018' '018' ... '018' '018' '018'\n",
       "  - voxel_num         (neuroid) int64 28 29 31 32 38 42 ... 152 153 154 159 160\n",
       "  - atlas             (neuroid) object 'language' 'language' ... 'language'\n",
       "  - filter_strategy   (neuroid) object '' '' '' '' '' '' ... '' '' '' '' '' ''\n",
       "  - atlas_selection   (neuroid) object 'from90to100prcnt' ... 'from90to100prcnt'\n",
       "  - roi               (neuroid) object 'LH_AntTemp' ... 'LH_AntTemp'\n",
       "  - indices_in_3d     (neuroid) int64 72505 72506 72584 ... 87754 87831 87832\n",
       "  - col_to_coord_1    (neuroid) int64 62 63 62 63 63 63 63 ... 65 62 63 64 62 63\n",
       "  - col_to_coord_2    (neuroid) int64 63 63 64 64 65 66 63 ... 65 66 66 66 67 67\n",
       "  - col_to_coord_3    (neuroid) int64 10 10 10 10 10 10 11 ... 12 12 12 12 12 12\n",
       "  - neuroid_id        (neuroid) object '018.28' '018.29' ... '018.159' '018.160'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark content\n",
    "benchmark._target_assembly.values.shape\n",
    "benchmark._target_assembly.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/rdma/vast-rdma/vast/cpl/ckauf/perturbed-neural-nlp/ressources/stimuli_creation\n",
      "/rdma/vast-rdma/vast/cpl/ckauf/perturbed-neural-nlp/ressources/scrambled_stimuli_dfs\n"
     ]
    }
   ],
   "source": [
    "stimuli_path = os.path.join(os.getcwd(),'ressources/stimuli_creation')\n",
    "os.chdir(stimuli_path)\n",
    "print(os.getcwd())\n",
    "\n",
    "savedir = abspath('../scrambled_stimuli_dfs')\n",
    "print(savedir)\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create different perturbed versions of the benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NOTE: \"Pereira2018_scrambled.txt\" is created via running \"get_original_sentenceset.ipynb\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_sentenceset(filename):\n",
    "    with open(os.path.join(stimuli_path,filename),\"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        sentences = list(reader)\n",
    "    Original = [sentence[1] for sentence in sentences if int(sentence[0]) == 0]\n",
    "    Original = [re.sub(r'[^\\w\\d\\s\\'\\-\\$\\%]+', '', sent.lower()) + \".\" for sent in Original]\n",
    "    \n",
    "    return Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beekeeping encourages the conservation of local habitats.', \"it is in every beekeeper's interest to conserve local plants that produce pollen.\", 'as a passive form of agriculture it does not require that native vegetation be cleared to make way for crops.', 'beekeepers also discourage the use of pesticides on crops because they could kill the honeybees.', 'artisanal beekeepers go to extremes for their craft but their product is worth the effort.']\n"
     ]
    }
   ],
   "source": [
    "#This is the list of stimuli we create the perturbations from\n",
    "Original = get_original_sentenceset(\"Pereira2018_scrambled.txt\")\n",
    "print(Original[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continue with Original instead of original sentence list from Pereira2018 for the following reason:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 | upon leaving the train station we saw our hotel looming above  a magnificent castle built on a cliff. | upon leaving the train station we saw our hotel looming above a magnificent castle built on a cliff.\n",
      "383 | a foot is a body part on the end of a leg . | a foot is a body part on the end of a leg.\n",
      "392 | forks are usually  made of metal or plastic if disposable. | forks are usually made of metal or plastic if disposable.\n",
      "434 | a knife can be used to attack by slashing stabbing or throwing . | a knife can be used to attack by slashing stabbing or throwing.\n",
      "504 | ravens feed on carrion insects berries or small animals . | ravens feed on carrion insects berries or small animals.\n",
      "506 | in folklore ravens are birds of ill-omen and also tricksters . | in folklore ravens are birds of ill-omen and also tricksters.\n",
      "562 | farmers often drain swamps to produce fertile arable land . | farmers often drain swamps to produce fertile arable land.\n",
      "******************************\n",
      "Adjusting for differences: \n",
      "double space for sent 209: upon leaving the train station we saw our hotel looming above  a magnificent castle built on a cliff.\n",
      "space before period for sent 383: a foot is a body part on the end of a leg .\n",
      "double space for sent 392: forks are usually  made of metal or plastic if disposable.\n",
      "space before period for sent 434: a knife can be used to attack by slashing stabbing or throwing .\n",
      "space before period for sent 504: ravens feed on carrion insects berries or small animals .\n",
      "space before period for sent 506: in folklore ravens are birds of ill-omen and also tricksters .\n",
      "space before period for sent 562: farmers often drain swamps to produce fertile arable land .\n",
      "******************************\n",
      "Changed!\n",
      "Asserted, now they're the same\n"
     ]
    }
   ],
   "source": [
    "# load sentences from dataset:\n",
    "pereira_sents = stimuli_df.sentence.values\n",
    "pereira_sents = [string.lower() for string in pereira_sents] #lowercase\n",
    "pereira_sents = [re.sub(r'[^\\w\\d\\s\\'\\-\\$\\%]+', '', sent.lower()) + \".\" for sent in pereira_sents] #strip punctuation\n",
    "\n",
    "#check for differences\n",
    "for ind in range(len(pereira_sents)):\n",
    "    if Original[ind] != pereira_sents[ind]:\n",
    "        print(f\"{ind} | {pereira_sents[ind]} | {Original[ind]}\")\n",
    "print(\"*\"*30)\n",
    "\n",
    "print(\"Adjusting for differences: \")\n",
    "pereira_sents = [string.lower() for string in pereira_sents] #lowecase\n",
    "\n",
    "#check & adjust for differences\n",
    "for ind, sent in enumerate(pereira_sents):\n",
    "    if \"  \" in sent:\n",
    "        print(f\"double space for sent {ind}: {sent}\")\n",
    "    elif \" .\" in sent:\n",
    "        print(f\"space before period for sent {ind}: {sent}\")\n",
    "    elif \". \" in sent:\n",
    "        print(f\"space after period for sent {ind}: {sent}\")\n",
    "    else:\n",
    "        continue\n",
    "print(\"*\"*30)\n",
    "pereira_sents = [re.sub(r' +', ' ', sent) for sent in pereira_sents] #strip double whitespace\n",
    "pereira_sents = [re.sub(r'\\. | \\.', '.', sent) for sent in pereira_sents] #strip whitespace before/after final period\n",
    "\n",
    "print(\"Changed!\")\n",
    "for ind in range(len(pereira_sents)):\n",
    "    if Original[ind] != pereira_sents[ind]:\n",
    "        print(f\"{ind} | {pereira_sents[ind]} | {Original[ind]}\")\n",
    "assert pereira_sents == Original\n",
    "print(\"Asserted, now they're the same\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation | O'Connor & Andreas (2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /home/ckauf/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('every', 'DT'),\n",
       " (\"beekeeper's\", 'NN'),\n",
       " ('interest', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('conserve', 'VB'),\n",
       " ('local', 'JJ'),\n",
       " ('plants', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('produce', 'VBP'),\n",
       " ('pollen.', 'NNS')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pos_tag_sentences(list_of_sentences):\n",
    "    \"\"\"Takes list of sentences as inputs and returns list of lists of POS-tags for each word within the sentence\n",
    "    \"\"\"\n",
    "    words = [re.split(r'\\s+', sent) for sent in list_of_sentences]\n",
    "    #don't use NLTK word tokenizer, or else build work-around for 's sentences\n",
    "    tagged = [nltk.pos_tag(word_list) for word_list in words]\n",
    "    #print(tagged[1:5])\n",
    "    return tagged\n",
    "    \n",
    "tagged = pos_tag_sentences(Original)\n",
    "tagged[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: words do not include dependent parts, like possessive markers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, sent in enumerate(tagged):\n",
    "    for tag_tuple in sent:\n",
    "        curr_word = tag_tuple[0].rstrip(\".\")\n",
    "        curr_tag = tag_tuple[1]\n",
    "        printing = False\n",
    "        if curr_tag == \"POS\":\n",
    "            printing = True\n",
    "        if printing is True:\n",
    "            print(ind, sent)\n",
    "            print(\"*\"* 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perturbed_datasets(sentences,perturb_type):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    * original sentence list (already lower-cased and stripped from punctuation for permute_sentences.py script)\n",
    "    * perturb_type = what should stay in the stimuli file?\n",
    "        nouns: only nouns\n",
    "        nounsverbs: only nouns and verbs\n",
    "        etc\n",
    "    Output:\n",
    "    * list of perturbed sentences\n",
    "    \"\"\"\n",
    "    tagged = pos_tag_sentences(sentences)\n",
    "    \n",
    "    n = ['NN.*', 'PRP.*'] #similar to O'Connor & Andreas (2021)\n",
    "    v = ['VB.*']\n",
    "    a = ['JJ.*']\n",
    "    adv = ['RB.*']\n",
    "    \n",
    "    if perturb_type == 'nouns':\n",
    "        pos_list = n\n",
    "    elif perturb_type == 'verbs':\n",
    "        pos_list = v\n",
    "    elif perturb_type == 'nounsverbs':\n",
    "        pos_list = n + v\n",
    "    elif perturb_type == 'nounsverbsadj':\n",
    "        pos_list = n + v + a\n",
    "    elif perturb_type == 'contentwords':\n",
    "        pos_list = n + v + a + adv\n",
    "    elif perturb_type == 'functionwords':\n",
    "        pos_list = n + v + a + adv #exclude in next step\n",
    "    else:\n",
    "        print(\"Unknown condition\")\n",
    "        \n",
    "    perturbed_sents = []\n",
    "    for sent in tagged:\n",
    "        if perturb_type != \"functionwords\": #if some kind of content words\n",
    "            \n",
    "            pert = ' '.join([tag_tuple[0].rstrip(\".\") for tag_tuple in sent if re.match(\"|\".join(pos_list), tag_tuple[1])]) + \".\"\n",
    "            perturbed_sents.append(pert)\n",
    "                \n",
    "        else:\n",
    "            pert = ' '.join([tag_tuple[0].rstrip(\".\") for tag_tuple in sent if not re.match(\"|\".join(pos_list), tag_tuple[1])]) + \".\"\n",
    "            perturbed_sents.append(pert)\n",
    "            \n",
    "    return perturbed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conservation habitats.',\n",
       " \"it beekeeper's interest plants pollen.\",\n",
       " 'form agriculture it vegetation way crops.',\n",
       " 'beekeepers use pesticides crops they honeybees.',\n",
       " 'beekeepers their craft their product effort.',\n",
       " 'honey-making quality character quantity consistency.',\n",
       " 'honey beekeepers micromanagers their honeybees.',\n",
       " 'they fields nectar ways honey.',\n",
       " 'beekeeper hum bees air.',\n",
       " 'beekeeper honey stores supplies bee nursery.',\n",
       " \"bees his bare arms hands they they're gentle.\",\n",
       " 'i dream exams college.',\n",
       " 'my dream day my exam i.',\n",
       " \"i've reading assignments.\",\n",
       " 'i people nightmare.',\n",
       " 'morning participants study their dream experience night.',\n",
       " 'they they dreams dream its intensity.',\n",
       " 'participants dream category dream dream nightmare.',\n",
       " 'night we our minds we.',\n",
       " 'we night cycles ninety minutes.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_sentences = get_perturbed_datasets(Original,perturb_type='nouns')\n",
    "noun_sentences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the of.',\n",
       " 'in every to that.',\n",
       " 'as a of that to for.',\n",
       " 'the of on because could the.',\n",
       " 'to for but worth the.',\n",
       " 'and over and.',\n",
       " 'to the of.',\n",
       " 'the when and the to.',\n",
       " 'as the the the of 40000 the.',\n",
       " 'the and the.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noun_sentences = get_perturbed_datasets(Original,perturb_type='functionwords')\n",
    "noun_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(stimuli_df, Original, perturb_type):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    * original benchmark dataframe\n",
    "    * original stimuli (list of sentences)\n",
    "    * perturbation type > what should stay in the stimuli?\n",
    "    Output:\n",
    "    * saves perturbed benchmark dataframe to save directory\n",
    "    \"\"\"\n",
    "    perturbed_sentences = get_perturbed_datasets(Original,perturb_type)\n",
    "    \n",
    "    perturbed_df = stimuli_df.copy()\n",
    "    perturbed_df[\"sentence\"] = perturbed_sentences\n",
    "\n",
    "    if save:\n",
    "        fname = f\"{savedir}/stimuli_{perturb_type}.pkl\"\n",
    "        with open(fname, 'wb') as fout:\n",
    "            pickle.dump(perturbed_df, fout)\n",
    "    return perturbed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets (mostly O'Connor & Andreas (2021))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset for perturbation type: contentwords\n",
      "Created dataset for perturbation type: nouns\n",
      "Created dataset for perturbation type: verbs\n",
      "Created dataset for perturbation type: nounsverbs\n",
      "Created dataset for perturbation type: nounsverbsadj\n",
      "Created dataset for perturbation type: functionwords\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>story</th>\n",
       "      <th>passage_index</th>\n",
       "      <th>passage_label</th>\n",
       "      <th>passage_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the of.</td>\n",
       "      <td>0</td>\n",
       "      <td>243sentences.0</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in every to that.</td>\n",
       "      <td>1</td>\n",
       "      <td>243sentences.1</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as a of that to for.</td>\n",
       "      <td>2</td>\n",
       "      <td>243sentences.2</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the of on because could the.</td>\n",
       "      <td>3</td>\n",
       "      <td>243sentences.3</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to for but worth the.</td>\n",
       "      <td>4</td>\n",
       "      <td>243sentences.4</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>2</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>some to.</td>\n",
       "      <td>379</td>\n",
       "      <td>384sentences.379</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.building_part</td>\n",
       "      <td>95</td>\n",
       "      <td>Window</td>\n",
       "      <td>building_part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>a a.</td>\n",
       "      <td>380</td>\n",
       "      <td>384sentences.380</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>a as a.</td>\n",
       "      <td>381</td>\n",
       "      <td>384sentences.381</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>a can and.</td>\n",
       "      <td>382</td>\n",
       "      <td>384sentences.382</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>a than a.</td>\n",
       "      <td>383</td>\n",
       "      <td>384sentences.383</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sentence  sentence_num       stimulus_id  \\\n",
       "0                         the of.             0    243sentences.0   \n",
       "1               in every to that.             1    243sentences.1   \n",
       "2            as a of that to for.             2    243sentences.2   \n",
       "3    the of on because could the.             3    243sentences.3   \n",
       "4           to for but worth the.             4    243sentences.4   \n",
       "..                            ...           ...               ...   \n",
       "622                      some to.           379  384sentences.379   \n",
       "623                          a a.           380  384sentences.380   \n",
       "624                       a as a.           381  384sentences.381   \n",
       "625                    a can and.           382  384sentences.382   \n",
       "626                     a than a.           383  384sentences.383   \n",
       "\n",
       "       experiment                       story  passage_index passage_label  \\\n",
       "0    243sentences     243sentences.beekeeping              1    beekeeping   \n",
       "1    243sentences     243sentences.beekeeping              1    beekeeping   \n",
       "2    243sentences     243sentences.beekeeping              1    beekeeping   \n",
       "3    243sentences     243sentences.beekeeping              1    beekeeping   \n",
       "4    243sentences     243sentences.beekeeping              2    beekeeping   \n",
       "..            ...                         ...            ...           ...   \n",
       "622  384sentences  384sentences.building_part             95        Window   \n",
       "623  384sentences          384sentences.human             96         Woman   \n",
       "624  384sentences          384sentences.human             96         Woman   \n",
       "625  384sentences          384sentences.human             96         Woman   \n",
       "626  384sentences          384sentences.human             96         Woman   \n",
       "\n",
       "    passage_category  \n",
       "0         beekeeping  \n",
       "1         beekeeping  \n",
       "2         beekeeping  \n",
       "3         beekeeping  \n",
       "4         beekeeping  \n",
       "..               ...  \n",
       "622    building_part  \n",
       "623            human  \n",
       "624            human  \n",
       "625            human  \n",
       "626            human  \n",
       "\n",
       "[627 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loop over perturbation types to create O'Connor & Andreas (2021) datasets\n",
    "perturb_types = [\"contentwords\", \"nouns\", \"verbs\", \"nounsverbs\", \"nounsverbsadj\", \"functionwords\"]\n",
    "\n",
    "for perturb in perturb_types:\n",
    "    perturb_df = get_dataset(stimuli_df, Original, perturb_type=perturb)\n",
    "    print(f\"Created dataset for perturbation type: {perturb}\")\n",
    "perturb_df   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create random noun replacement condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>stimulus_id</th>\n",
       "      <th>experiment</th>\n",
       "      <th>story</th>\n",
       "      <th>passage_index</th>\n",
       "      <th>passage_label</th>\n",
       "      <th>passage_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>its doctors.</td>\n",
       "      <td>0</td>\n",
       "      <td>243sentences.0</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>images ligaments chamber documentary jacket.</td>\n",
       "      <td>1</td>\n",
       "      <td>243sentences.1</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>factor compartments world spears density studies.</td>\n",
       "      <td>2</td>\n",
       "      <td>243sentences.2</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>horror computer sea attack food collar.</td>\n",
       "      <td>3</td>\n",
       "      <td>243sentences.3</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>1</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dreams their glimpse water firearm forest.</td>\n",
       "      <td>4</td>\n",
       "      <td>243sentences.4</td>\n",
       "      <td>243sentences</td>\n",
       "      <td>243sentences.beekeeping</td>\n",
       "      <td>2</td>\n",
       "      <td>beekeeping</td>\n",
       "      <td>beekeeping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>automobile he bear vapor.</td>\n",
       "      <td>379</td>\n",
       "      <td>384sentences.379</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.building_part</td>\n",
       "      <td>95</td>\n",
       "      <td>Window</td>\n",
       "      <td>building_part</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>pianist people.</td>\n",
       "      <td>380</td>\n",
       "      <td>384sentences.380</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>law freezes.</td>\n",
       "      <td>381</td>\n",
       "      <td>384sentences.381</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>driver villages.</td>\n",
       "      <td>382</td>\n",
       "      <td>384sentences.382</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>cats nursery weapon.</td>\n",
       "      <td>383</td>\n",
       "      <td>384sentences.383</td>\n",
       "      <td>384sentences</td>\n",
       "      <td>384sentences.human</td>\n",
       "      <td>96</td>\n",
       "      <td>Woman</td>\n",
       "      <td>human</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>627 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  sentence_num  \\\n",
       "0                                         its doctors.             0   \n",
       "1         images ligaments chamber documentary jacket.             1   \n",
       "2    factor compartments world spears density studies.             2   \n",
       "3              horror computer sea attack food collar.             3   \n",
       "4           dreams their glimpse water firearm forest.             4   \n",
       "..                                                 ...           ...   \n",
       "622                          automobile he bear vapor.           379   \n",
       "623                                    pianist people.           380   \n",
       "624                                       law freezes.           381   \n",
       "625                                   driver villages.           382   \n",
       "626                               cats nursery weapon.           383   \n",
       "\n",
       "          stimulus_id    experiment                       story  \\\n",
       "0      243sentences.0  243sentences     243sentences.beekeeping   \n",
       "1      243sentences.1  243sentences     243sentences.beekeeping   \n",
       "2      243sentences.2  243sentences     243sentences.beekeeping   \n",
       "3      243sentences.3  243sentences     243sentences.beekeeping   \n",
       "4      243sentences.4  243sentences     243sentences.beekeeping   \n",
       "..                ...           ...                         ...   \n",
       "622  384sentences.379  384sentences  384sentences.building_part   \n",
       "623  384sentences.380  384sentences          384sentences.human   \n",
       "624  384sentences.381  384sentences          384sentences.human   \n",
       "625  384sentences.382  384sentences          384sentences.human   \n",
       "626  384sentences.383  384sentences          384sentences.human   \n",
       "\n",
       "     passage_index passage_label passage_category  \n",
       "0                1    beekeeping       beekeeping  \n",
       "1                1    beekeeping       beekeeping  \n",
       "2                1    beekeeping       beekeeping  \n",
       "3                1    beekeeping       beekeeping  \n",
       "4                2    beekeeping       beekeeping  \n",
       "..             ...           ...              ...  \n",
       "622             95        Window    building_part  \n",
       "623             96         Woman            human  \n",
       "624             96         Woman            human  \n",
       "625             96         Woman            human  \n",
       "626             96         Woman            human  \n",
       "\n",
       "[627 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_random_noun_dataset(sentences):\n",
    "    \"\"\"\n",
    "    Input: list of original sentences\n",
    "    Output: perturbed version of each sentence with only nouns (same number as in original sentence),\n",
    "    but randomly drawn from dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    #set random seed for reproducability\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    tagged = pos_tag_sentences(sentences)\n",
    "    \n",
    "    n = ['NN.*', 'PRP.*'] #similar to O'Connor & Andreas (2021)\n",
    "    pos_list = n\n",
    "    \n",
    "    #gather all nouns in a list\n",
    "    all_nouns = []\n",
    "    #count how many nouns should go in each sentence\n",
    "    nr_nouns_in_sentences = []\n",
    "    #keep track of which nouns were in which sentence for checking later\n",
    "    nouns_in_sentences = []\n",
    "    \n",
    "    for sent in tagged:\n",
    "        curr_nouns = [tag_tuple[0].rstrip(\".\") for tag_tuple in sent if re.match(\"|\".join(pos_list), tag_tuple[1])]\n",
    "        \n",
    "        all_nouns += curr_nouns\n",
    "        nr_nouns_in_sentences.append(len(curr_nouns))\n",
    "        nouns_in_sentences.append(curr_nouns)\n",
    "    \n",
    "    perturbed_sentences = []\n",
    "    for ind, n in enumerate(nr_nouns_in_sentences):\n",
    "        random_nouns = random.sample(all_nouns, n)\n",
    "        assert set(random_nouns) != set(nouns_in_sentences[ind]) #check that not the same nouns are selected\n",
    "        perturbed_sentences.append(' '.join(random_nouns) + \".\")\n",
    "        [all_nouns.remove(elm) for elm in random_nouns] #remove selected nouns from list\n",
    "        #print(len(all_nouns))\n",
    "    \n",
    "    assert len(all_nouns) == 0, f\"Not all words from the dataset have been used. Length of word list is {len(all_words)}!\"\n",
    "     \n",
    "    return perturbed_sentences\n",
    "\n",
    "random_nouns = get_random_noun_dataset(Original)\n",
    "\n",
    "perturbed_df = stimuli_df.copy()\n",
    "perturbed_df[\"sentence\"] = random_nouns\n",
    "\n",
    "if save:\n",
    "    fname = f\"{savedir}/stimuli_randomnouns.pkl\"\n",
    "    with open(fname, 'wb') as fout:\n",
    "        pickle.dump(perturbed_df, fout)\n",
    "\n",
    "perturbed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#check that nouns and random nouns datasets have same length strings:\n",
    "check_randomnouns_df = perturbed_df\n",
    "check_nouns_df = get_dataset(stimuli_df, Original, perturb_type=\"nouns\")\n",
    "    \n",
    "sent_len_randomnouns = [len(elm.split()) for elm in list(check_randomnouns_df[\"sentence\"])]\n",
    "sent_len_nouns = [len(elm.split()) for elm in list(check_nouns_df[\"sentence\"])]\n",
    "\n",
    "assert sent_len_randomnouns == sent_len_nouns\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
